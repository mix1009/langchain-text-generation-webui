{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d162b55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T02:50:35.374852Z",
     "start_time": "2023-05-07T02:50:34.240588Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Generator, List, Optional\n",
    "import requests\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "class TextGenerationWebUI(LLM):\n",
    "    \"\"\"Wrapper for text-generation-webui API.\"\"\"\n",
    "\n",
    "    max_new_tokens: int = 200\n",
    "    \"\"\"maximum number of tokens to generate.\"\"\"\n",
    "\n",
    "    temperature: float = 0.7\n",
    "    \"\"\"Primary factor to control randomness of outputs. 0 = deterministic (only the most likely token is used). Higher value = more randomness.\"\"\"\n",
    "\n",
    "    top_p: float = 1\n",
    "    \"\"\"If not set to 1, select tokens with probabilities adding up to less than this number. Higher value = higher range of possible random results.\"\"\"\n",
    "\n",
    "    top_k: int = 40\n",
    "    \"\"\"Similar to top_p, but select instead only the top_k most likely tokens. Higher value = higher range of possible random results.\"\"\"\n",
    "\n",
    "    typical_p: float = 1.0\n",
    "    \"\"\"If not set to 1, select only tokens that are at least this much more likely to appear than random tokens, given the prior text.\"\"\"\n",
    "\n",
    "    repetition_penalty: float = 1.2\n",
    "    \"\"\"Exponential penalty factor for repeating prior tokens. 1 means no penalty, higher value = less repetition, lower value = more repetition.\"\"\"\n",
    "    \n",
    "    encoder_repetition_penalty: float = 1.0\n",
    "    \"\"\"Also known as the \"Hallucinations filter\". Used to penalize tokens that are *not* in the prior text. Higher value = more likely to stay in context, lower value = more likely to diverge.\"\"\"\n",
    "\n",
    "    no_repeat_ngram_size: int = 0\n",
    "    \"\"\"If not set to 0, specifies the length of token sets that are completely blocked from repeating at all. Higher values = blocks larger phrases, lower values = blocks words or letters from repeating. Only 0 or high values are a good idea in most cases.\"\"\"\n",
    "\n",
    "    min_length: int = 0\n",
    "    \"\"\"Minimum generation length in tokens.\"\"\"\n",
    "\n",
    "    do_sample: bool = True\n",
    "\n",
    "    seed: int = -1\n",
    "    \"\"\"seed: -1 for random\"\"\"\n",
    "\n",
    "    penalty_alpha: float = 0.0\n",
    "    \"\"\"Contrastive search\"\"\"\n",
    "\n",
    "    num_beams: int = 1\n",
    "    \"\"\"Beam search (uses a lot of VRAM)\"\"\"\n",
    "\n",
    "    length_penalty: float = 1.0\n",
    "    \"\"\"Beam search length penalty\"\"\"\n",
    "        \n",
    "    early_stopping: bool = False\n",
    "        \n",
    "    truncation_length: int = 2048\n",
    "    \"\"\"The leftmost tokens are removed if the prompt exceeds this length. Most models require this to be at most 2048.\"\"\"\n",
    "\n",
    "    stop: List[str] = None\n",
    "    \"\"\"Custom stopping strings\"\"\"\n",
    "\n",
    "    add_bos_token: bool = True\n",
    "    \"\"\"Add the bos_token to the beginning of prompts. Disabling this can make the replies more creative.\"\"\"\n",
    "        \n",
    "    ban_eos_token: bool = False\n",
    "    \"\"\"Ban the eos_token. Forces the model to never end the generation prematurely.\"\"\"\n",
    "    \n",
    "    skip_special_tokens: bool = True\n",
    "    \"\"\"Skip special tokens. Some specific models need this unset.\"\"\"\n",
    "    \n",
    "    api_host: str = \"localhost\"\n",
    "    api_port: int = 5000\n",
    "    api_streaming_port: int = 5005\n",
    "    use_https: bool = False\n",
    "\n",
    "#     @root_validator()\n",
    "#     def validate_environment(cls, values: Dict) -> Dict:\n",
    "#         \"\"\"Validate\"\"\"\n",
    "#         return values\n",
    "\n",
    "    @property\n",
    "    def _default_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get the default parameters.\"\"\"\n",
    "        return {\n",
    "            \"max_new_tokens\": self.max_new_tokens,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"top_p\": self.top_p,\n",
    "            \"top_k\": self.top_k,\n",
    "            \"repetition_penalty\": self.repetition_penalty,\n",
    "            \"encoder_repetition_penalty\": self.encoder_repetition_penalty,\n",
    "            \"typical_p\": self.typical_p,\n",
    "            \"min_length\": self.min_length,\n",
    "            \"no_repeat_ngram_size\": self.no_repeat_ngram_size,\n",
    "            \"num_beams\": self.num_beams,\n",
    "            \"length_penalty\": self.length_penalty,\n",
    "            \"penalty_alpha\": self.penalty_alpha,\n",
    "            \"early_stopping\": self.early_stopping,\n",
    "            \"seed\": self.seed,\n",
    "            \"add_bos_token\": self.add_bos_token,\n",
    "            \"ban_eos_token\": self.ban_eos_token,\n",
    "            \"truncation_length\": self.truncation_length,\n",
    "            \"skip_special_tokens\": self.skip_special_tokens,\n",
    "            \"do_sample\": self.do_sample,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return self._default_params\n",
    "#         return {**{\"model_path\": self.model_path}, **self._default_params}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of llm.\"\"\"\n",
    "        return \"text-generation-webui\"\n",
    "\n",
    "    def _get_parameters(self, stop: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "\n",
    "        params = self._default_params\n",
    "        params[\"stop\"] = self.stop or stop or []\n",
    "        params[\"stopping_strings\"] = params[\"stop\"]\n",
    "\n",
    "        return params\n",
    "    \n",
    "    def get_base_url(self):\n",
    "        proto = \"https\" if self.use_https else \"http\"\n",
    "        return f\"{proto}://{self.api_host}:{self.api_port}\"\n",
    "    \n",
    "    def get_model_name(self):\n",
    "        URI = f'{self.get_base_url()}/api/v1/model'\n",
    "        response = requests.get(URI)\n",
    "        return response.json()['result']\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Call the text-generation-webui API and return the output.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to use for generation.\n",
    "            stop: A list of strings to stop generation when encountered.\n",
    "\n",
    "        Returns:\n",
    "            The generated text.\n",
    "\n",
    "        \"\"\"\n",
    "        request = self._get_parameters(stop) \n",
    "        generations = []\n",
    "        \n",
    "        request['prompt'] = prompt\n",
    "\n",
    "        URI = f'{self.get_base_url()}/api/v1/generate'\n",
    "        response = requests.post(URI, json=request)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()['results'][0]['text']\n",
    "            return result\n",
    "        \n",
    "    def get_token_count(self, prompt):\n",
    "        URI = f'{self.get_base_url()}/api/v1/token-count'\n",
    "        request = {}\n",
    "        request['prompt'] = prompt\n",
    "        response = requests.post(URI, json=request)\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()['results'][0]['tokens']\n",
    "            return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "944124ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T02:51:51.308146Z",
     "start_time": "2023-05-07T02:51:49.252137Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TheBloke_wizardLM-7B-GPTQ'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = TextGenerationWebUI(temperature=0.1)\n",
    "\n",
    "llm.get_model_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47b1f3c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T02:52:00.085217Z",
     "start_time": "2023-05-07T02:51:56.379360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \"Unlocking the Power of Large Language Models with AI\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"A chat between a human and an assistant.\n",
    "\n",
    "### Human: write a youtube video title about Large Language Model.\n",
    "\n",
    "TITLE:\n",
    "### Assistant:\"\"\"\n",
    "\n",
    "result = llm(prompt, stop=[\"\\n### Human:\", \"\\n### Assistant:\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55d40c15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T02:52:08.709799Z",
     "start_time": "2023-05-07T02:52:06.658130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_token_count(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1852b247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
